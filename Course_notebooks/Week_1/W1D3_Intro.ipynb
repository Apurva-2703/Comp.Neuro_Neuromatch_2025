{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8JAv6owNcjzj"
      },
      "source": [
        "# Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Priming for Day 3 - Intro video by Christina Savin\n",
        "\n",
        "## Core concepts\n",
        "\n",
        "* GLM has three components that takes the multidimensional input to give scalar output:\n",
        "  - linear filter: maps multidimensional input into a scalar output\n",
        "  - nonlinearity\n",
        "  - noise (from exponential family)\n",
        "\n",
        "* Different GLMs can solve different problems:\n",
        "  - Linear regression uses Gaussian noise and Identity function (as non-linearity) to output real values.\n",
        "  - Poisson GLM uses poisson noise and exponential function to output discrete counts.\n",
        "  - Logistic regression uses Bernoulli distribution as noise and a logistic function to give binary outputs.\n",
        "\n",
        "* We perform parameter learning optimizing Maximum likelihood estimate (MLE), which for a poisson GLM consists of maximizing the log-likelihood, which is equivalent to minimizing the negative log-likelihood.\n",
        "  - Note: MLE Gives us the best training data but it might be overfitting the test data.\n",
        "  - Cross-validation finds the best hyperparameter (e.g., λ) by measuring generalization error — e.g., “Which λ gives the lowest test MSE?”\n",
        "\n",
        "* Regularizers:\n",
        "  - L2: keep weights small\n",
        "  - L1: encourage sparsity\n",
        "\n",
        "## Variables (Inputs/Outputs)\n",
        "\n",
        "* ENCODE = input: stimuli, output: neural response\n",
        "* DECODE = input: neural activity, output: behavior\n",
        "* GLM = linear filter + nonlinear function + exponential noise = scalar output\n",
        "\n",
        "## Assumptions\n",
        "\n",
        "* Neural activity can represent stimuli and inform behavior.\n",
        "* L2 regularizers assume many small weights\n",
        "* L1 regularizes assume a few non-zero weights.\n",
        "\n",
        "## Vocabulary flags\n",
        "\n",
        "* Generalized Linear Models\n",
        "* Encoding (forward model)\n",
        "* Functional connectivity\n",
        "* Decoding: Given neural responses, predict the decision (y)\n",
        "* Decoding (reverse model)\n",
        "* Bernouilli distribution\n",
        "* Overfitting\n",
        "* concave log-likelihood\n",
        "* Regularization\n",
        "* Hyperparameter"
      ],
      "metadata": {
        "id": "hfHoTdrWdJxa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0-_X4KWJcjzl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Hi folks, welcome to the ‘GLM’ day, the highlight of neuromatch (really)!\n",
        "\n",
        "We will talk about generalized linear models, the so-called GLMs. The great thing about GLMs is that they provide a simple and elegant framework that unifies several useful ML algorithms and allow you to tackle a wide range of data analysis questions in neuroscience. Think of them as the Swiss Army Knife of Machine Learning algorithms.\n",
        "\n",
        "In particular, some algorithms you might know, such as linear regression, logistic regression, and Poisson regression, are all just special cases of GLMs.  They are very useful if you want to study encoding (i.e., predict neural activity from external covariates) or decoding (i.e., predict a behavior or intention from neural activity).\n",
        "\n",
        "In addition, GLMs are great for teaching some general concepts in machine learning that are useful to know-- optimization of cost functions (and what is means for a cost function to be convex, and why that is useful), what overfitting is, how to spot it (cross-validation!) and how to protect against it (regularization!), what types of regularization exist (L1, L2), and how this connects to Reverend Bayes.\n",
        "\n",
        "Finally -- and just like a Swiss Army Knife’, GLMs have limitations, and sometimes we will need more powerful methods-- however, even in those cases, GLMs provide a baseline against which more complicated algorithms (e.g., deep nets) can be compared. In addition, GLMs can be used as building blocks for constructing more complicated algorithms.\n",
        "\n",
        "The day is structured as follows:\n",
        "In the ‘Intro’, Dr. Christina Savin will first give an overview of canonical analysis tasks in neuroscience and show what they have in common and how they can be tackled with GLMs. She will explain one task (receptive field fitting) as a running example and use it to explain concepts like optimization, overfitting, and regularization.\n",
        "\n",
        "In the first part of Tutorial 1, Dr. Anqi Qu (now an assistant prof at Georgia Tech!) will delve deeper into our running example (receptive field estimation), and go through the details of the mathematics of this task, and show that spike-triggered averaging (a great but often heuristically motivated approach) is closely related to a special case of GLMs-- the good old linear regression. In the second part of Tutorial 2, she will show how ‘Poisson GLMs’ is a really powerful way to fit encoding models to spiking data.\n",
        "\n",
        "In Tutorial 3, she will turn to logistic regression (my favorite classification algorithm) and show how it can be used for decoding. She will also explain regularization and the difference between different regularizers.\n",
        "In the ‘Outro’, Dr. Memming Park will first tell you about his cat, and go into much more detail  about applications of GLMs in neuroscience- after a general overview, he will talk about how GLMs can be used to model the dynamics of single neurons and neural population dynamics, and how they can be used to model the responses of neurons in decision-making tasks and to decode behavioural choices from neurons. Finally-- but importantly-- he will talk about some potential pitfalls in interpreting GLMs, particularly when the danger of (falsely) interpreting their parameters in a causal manner. In the end, he will touch on some possible generalizations of GLMs.\n",
        "\n",
        "Regarding the overall course, the closest connections are with the day of ‘model fitting’, which will already introduce some of the basic ideas. Once you have been through the GLM day, you will then likely also notice the use of GLMs in many other days and tasks (even if they might not always be referred to as ‘GLMs’), e.g., for decoding behavioral data, or as building blocks as models of dynamics.\n",
        "\n",
        "So, enjoy the day!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Qs5X5JDmcjzl"
      },
      "source": [
        "## Prerequisite knowledge\n",
        "\n",
        "In today's tutorials, you will use matrix vector multiplication ([W0D3 T2](https://compneuro.neuromatch.io/tutorials/W0D3_LinearAlgebra/student/W0D3_Tutorial2.html)), probability distributions ([W0D5 T1](https://compneuro.neuromatch.io/tutorials/W0D5_Statistics/student/W0D5_Tutorial1.html)), and maximum likelihood estimates ([W0D5 T2](https://compneuro.neuromatch.io/tutorials/W0D3_LinearAlgebra/student/W0D3_Tutorial2.html) and [W1D3](https://compneuro.neuromatch.io/tutorials/W1D3_ModelFitting/chapter_title.html)).  You will also be looking at stimulus response pairings ([Neuro Video Series Stimulus Representation](https://compneuro.neuromatch.io/tutorials/W0D0_NeuroVideoSeries/student/W0D0_Tutorial10.html)) and decision making experiments in animals ([Neuro Video Series Behavioural Readouts](https://compneuro.neuromatch.io/tutorials/W0D0_NeuroVideoSeries/student/W0D0_Tutorial3.html)). You will use a new Python library called [sklearn](https://scikit-learn.org/stable/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "W1D3_Intro",
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}