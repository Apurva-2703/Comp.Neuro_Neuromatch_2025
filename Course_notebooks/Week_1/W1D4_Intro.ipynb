{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "-WL0Gi5sfKub"
      },
      "source": [
        "# Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Priming for Day 4 - Intro video on Dimensionality Reduction by Byron Yu"
      ],
      "metadata": {
        "id": "GF2oRG5ufOMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core concepts\n",
        "\n",
        "* Given that neurons form networks, no neuron can act independently. So, the brain has fewer degrees of freedom (if we consider different networks) than the number of neurons.\n",
        "  * Therefore, dimensionality reduction consists of identifying these principal components that modulate the behavior such that we take high-dimension observations and classify them according to low-dimension states.\n",
        "\n",
        "* Dimensionality reduction is used in plenty of areas in neuroscience to study behavioral encoding and even applying to decoding in the form of BCI. There are multiple methods that we can use:\n",
        "  - Principal component analyes (trial-average analyes; no concept of noise)\n",
        "  - Factor analyses (single-trial analyses - captures variance shared amongst neurons; no temporal smoothing)\n",
        "  - Gaussian-process factor analyses (single-trial analyses - has temporal smoothing)\n",
        "  - Latent dynamical systems [LDS, LFADS] - incorporate dynamical rules governing time-evolution of neural activity\n",
        "  - Non-linear methods [Isomap, LLE, t-SNE] - multiple assumptions like dense sampling, low-noise.. often not satisfied with neural activity\n",
        "  - Supervised methods [LDA, dPCA] - identify dimensions that represent stimulus, behavior, or time\n",
        "\n",
        "* Reasons to use dimensionality reduction:\n",
        "  - Single-trial analyses of neural population activity\n",
        "  - Hypotheses about population activity structure (How neurons co-vary each other)\n",
        "  - Exploratory analyses of large datasets\n",
        "\n",
        "## Variables\n",
        "* Noisy time-series (H-D) -- (temporal smoothing, dimensionality reduction) --> denoised time series = neural trajectory (L-D)\n",
        "\n",
        "## Vocabulary Flags\n",
        "\n",
        "* low-dimension states\n",
        "* high-dimension observation/activity (H-D)\n",
        "* temporal smoothing\n",
        "* GPFA\n",
        "* FA\n",
        "* PCA\n",
        "* LDS\n",
        "* LLE\n",
        "* t-SNE\n",
        "* DataHigh software - visualize multiple(n) 3D projections of n-dimension data"
      ],
      "metadata": {
        "id": "DmwFFiuqfe3x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "u91TFqlRfKud"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Today, we'll learn all about dimensionality reduction, a machine learning technique that we use all the time in neuroscience! Dimensionality reduction means transforming data from a high-dimensional space into a lower-dimensional space. In the intro, Byron Yu will introduce you to several ways to perform dimensionality reduction and discuss how they're used to investigate the possible underlying low-dimensional structure of neural population activity. Tutorial 1 sets up the foundational knowledge for the main type of dimensionality reduction we'll cover: Principal Components Analysis (PCA). In particular, it will review key linear algebra components such as orthonormal bases, changing bases, and correlation. Tutorial 2 then covers the specific math behind PCA: how we compute it and project data onto the principal components. Tutorial 3 covers how we assess how many dimensions (or principal components) we need to represent the data accurately. Finally, in Tutorial 4, you will briefly be introduced to a second dimensionality reduction method, a non-linear method called t-SNE. You'll hear from Byron Yu again in the outro, where he will connect dimensionality reduction to brain-computer interfaces.\n",
        "\n",
        "Dimensionality reduction is a core machine learning technique used constantly in neuroscience for various reasons. Neuroscientists use it as a simple data analysis tool to compress data into a lower number of dimensions better suited for further analysis and modeling. For example, decoding all pixels of an image from neural data is difficult as that is 100s to 1000s of dimensions. We could instead compress to a lower-dimensional version of those images and decode that using the methods and models we've already learned about (such as linear regression in *Model Fitting*). We can also visualize data better in lower dimensions - you'll use PCA to visualize the internal representations of a deep network and mouse brain in *Deep Learning*. We can also use dimensionality reduction to think about and investigate low-dimensional structure in the brain - whether it exists, how it is formed, and so on. You'll see more insight into this if you look through the bonus day materials on *Autoencoders*.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "W1D4_Intro",
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}