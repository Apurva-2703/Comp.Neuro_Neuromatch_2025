{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pF9cwSP0qeoi",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Model fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BL_0UOdq08T"
      },
      "source": [
        "# Priming for Day 2 - Intro video\n",
        "\n",
        "## Core concepts: Few big ideas that are being introduced.\n",
        "* Two central questions in science:\n",
        "  * How should we pick the parameters from models\n",
        "  * Which models explain reality the best?\n",
        "  \n",
        "\n",
        "* Model fitting = Get the best parameters for a given model.\n",
        "  * Minimizing Mean Squared Errors. (Tutorial 1)\n",
        "  * Maxmium Likelihood Estimates. (Tutorial 2)\n",
        "    * Note that minimizing MSE = maximizing the log-likelihood while accounting for gaussian noise (however, this criteria is more sensitive to outliers).\n",
        "    * For linear models = *[ log L(Î˜|X,y) ]*\n",
        "\n",
        "* Model evaluation = How well does this model fit the data?\n",
        "  * Goodness of fit - log-likelihood* (We used this for model fitting in our Tutorial 2)\n",
        "  * Cross validation - Fit model to some data and then check how well it predicts new data. (Tutorial 5)\n",
        "\n",
        "* Model comparison = Which model is better?\n",
        "  * Bias-and-variance trade-off (Tutorial 5)\n",
        "  * Akaike Information Criterion (AIC) = 2k - 2log(L) where k is the number of parameters and L is likelihood. This penalizes complexity.\n",
        "  * Cross-validation performance (Tutorial 6)\n",
        "\n",
        "* Parameter uncertainty = How confident are we in these parameters?\n",
        "  * Bootstrapping and confidence intervals for each parameter. (Tutorial 3)\n",
        "\n",
        "\n",
        "## Variables or data inputs? Outputs?\n",
        "* Linear models can be used for validation (new-data, held-out data), prediction, interpreting, and comparison across different models.\n",
        "* Likelihood functions take the data in question and evaluate how likely it is given the parameters of the model. In here, we aimm for maximum likelihood (ML) fits.\n",
        "* For encoding models, inputs - stimulus features. outputs = predicted spike rates.\n",
        "* In GLMs, we extend linear models using non-linear link functions.\n",
        "\n",
        "## Assumptions: What are the assumptions being made for modelling?\n",
        "* Linear models are linear in parameters, not necessarily in the inputs.\n",
        "  * For example, orientation/contract/color can all affect a spike *but* in a linear model, they woulld all independently and additively contribute to the model.\n",
        "* Generative perspective on model fitting assumes that the model \"generates\" the data and variation is simply _noise_ - so this tool for testing model fit is sensitive to outliers.\n",
        "* AIC assumes model structures.\n",
        "* Cross-validation assumes lots of data, has less sensitivity to small differences, and is computeationally expensive.\n",
        "\n",
        "# Vocabulary flags\n",
        "* Fitting (linear) models\n",
        "* Maximum likelihood (ML)\n",
        "* Mean Squared Error (MSE)\n",
        "* Gaussian Noise\n",
        "* Bootstrapping\n",
        "* Bias-error trade-off\n",
        "* AIC\n",
        "* Goodness of fit\n",
        "* Cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "nbuaOnwtqeok",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Overview\n",
        "\n",
        "Today we will talk about how to confront models with experimental data. Does my model capture the behavior of my participant or its neural activity? Does the data support my model against alternatives? Which component in the model is needed? Do parameters in the model vary systematically between two subject populations? We will cover the basic concepts and tools to address these questions, starting with a general overview in the intro. You will learn how to estimate the parameters of simple regression models from the data in Tutorials 1 & 2 and then how to estimate the uncertainty about these values in Tutorial 3. Then you will learn how to select from models of different complexity the one that best accounts for your data (Tutorial 4-6). The outro illustrates some of these techniques in real research examples.\n",
        "\n",
        "Fitting and comparing models to data is really the bread and butter of data analysis in neuroscience. During Model Types Day, you learned about a whole zoo of different types of models that we're interested in neuroscience. So here, you will learn about generic concepts and techniques that apply for fitting and comparing any type of model, which is arguably pretty useful! You will apply these tools again when dealing with GLMs, latent models, deep networks, dynamical models, decision models, reinforcement learning models... it's everywhere! On top of this, we will cover linear regression models, the typical regression model when the dependent variable is continuous (e.g., BOLD activity), and use it throughout the day to illustrate the concepts and methods we learn. On the GLM day, you will see how to generalize regression models when the dependent variable is binary (e.g., choices) or an integer (e.g., spike counts).\n",
        "\n",
        "Almost all statistical and data analysis methods rely either explicitly or implicitly on fitting some model to the data. The concepts and tools you will learn today are crucial to be able to test your hypothesis about how behavior or neural activity is formed. A typical way this is done is that you formulate a computational (stochastic) model that embodies your hypothesis and then one or more control models. Then you fit each of your models to your experimental data, say the pattern of choices of one experimental subject or the spiking activity from one recorded neuron. Simulating your fitted models allows validating that your model indeed captures the effects of interest in your data. Then you use model comparison techniques to tell which one of your main or control model(s) provides a better description of the data. Also, you could assess whether some parameter in your model changes between experimental conditions or subject populations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "cXU8I0oSqeok"
      },
      "source": [
        "## Prerequisite knowledge\n",
        "\n",
        "In the content and tutorials today, you will be using vectors and matrices ([W0D3 T1](https://precourse.neuromatch.io/tutorials/W0D3_LinearAlgebra/student/W0D3_Tutorial1.html) & [T2](https://precourse.neuromatch.io/tutorials/W0D3_LinearAlgebra/student/W0D3_Tutorial2.html)), probability distributions ([W0D5 T1](https://precourse.neuromatch.io/tutorials/W0D5_Statistics/student/W0D5_Tutorial1.html)), and likelihoods ([W0D5 T2](https://precourse.neuromatch.io/tutorials/W0D5_Statistics/student/W0D5_Tutorial2.html)).  Please review this material if necessary!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "W1D2_Intro",
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
